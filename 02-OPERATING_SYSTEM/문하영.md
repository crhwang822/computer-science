
<details>
  <summary><h3>1. 시스템 콜이 무엇인지 설명해 주세요.</h3></summary>
<ul>
<li> 우리가 사용하는 시스템 콜의 예시를 들어주세요.</li>
<li> 시스템 콜이, 운영체제에서 어떤 과정으로 실행되는지 설명해 주세요.</li>
<li> 시스템 콜의 유형에 대해 설명해 주세요.</li>
<li> 운영체제의 Dual Mode 에 대해 설명해 주세요.</li>
<li> 왜 유저모드와 커널모드를 구분해야 하나요? </li>
<li> 서로 다른 시스템 콜을 어떻게 구분할 수 있을까요?</li>
</ul>
</details>

<details>
  <summary><h3>2. 인터럽트가 무엇인지 설명해 주세요.</h3></summary>
<ul>
<li> 인터럽트는 어떻게 처리하나요?</li>
<li> Polling 방식에 대해 설명해 주세요.</li>
<li> HW / SW 인터럽트에 대해 설명해 주세요.</li>
<li> 동시에 두 개 이상의 인터럽트가 발생하면, 어떻게 처리해야 하나요? </li>
</ul>
</details>

<details>
  <summary><h3>3. 프로세스가 무엇인가요?</h3></summary>
<ul>
<li> 프로그램과 프로세스, 스레드의 차이에 대해 설명해 주세요.</li>
<li> PCB가 무엇인가요?</li>
<li> 그렇다면, 스레드는 PCB를 갖고 있을까요?</li>
<li> 리눅스에서, 프로세스와 스레드는 각각 어떻게 생성될까요?</li>
<li> 자식 프로세스가 상태를 알리지 않고 죽거나, 부모 프로세스가 먼저 죽게 되면 어떻게 처리하나요?</li>
<li> 리눅스에서, 데몬프로세스에 대해 설명해 주세요.</li>
<li> 리눅스는 프로세스가 일종의 트리를 형성하고 있습니다. 이 트리의 루트 노드에 위치하는 프로세스에 대해 설명해 주세요.</li>
</ul>
</details>

<details>
  <summary><h3>4. 프로세스 주소공간에 대해 설명해 주세요.</h3></summary>
<ul>
<li> 초기화 하지 않은 변수들은 어디에 저장될까요?</li>
<li> 일반적인 주소공간 그림처럼, Stack과 Heap의 크기는 매우 크다고 할 수 있을까요? 그렇지 않다면, 그 크기는 언제 결정될까요?</li>
<li> Stack과 Heap 공간에 대해, 접근 속도가 더 빠른 공간은 어디일까요?</li>
<li> 다음과 같이 공간을 분할하는 이유가 있을까요?</li>
<li> 스레드의 주소공간은 어떻게 구성되어 있을까요?</li>
<li> "스택"영역과 "힙"영역은 정말 자료구조의 스택/힙과 연관이 있는 걸까요? 만약 그렇다면, 각 주소공간의 동작과정과 연계해서 설명해 주세요.</li>
<li> IPC의 Shared Memory 기법은 프로세스 주소공간의 어디에 들어가나요? 그런 이유가 있을까요? </li>
<li> 스택과 힙영역의 크기는 언제 결정되나요? 프로그램 개발자가 아닌, 사용자가 이 공간의 크기를 수정할 수 있나요? </li>
</ul>
</details>

<details>
  <summary><h3>5. 단기, 중기, 장기 스케쥴러에 대해 설명해 주세요.</h3></summary>
<ul>
<li> 현대 OS에는 단기, 중기, 장기 스케쥴러를 모두 사용하고 있나요?</li>
<li> 프로세스의 스케쥴링 상태에 대해 설명해 주세요.</li>
<li> preemptive/non-preemptive 에서 존재할 수 없는 상태가 있을까요?</li>
<li> Memory가 부족할 경우, Process는 어떠한 상태로 변화할까요?</li>
</ul>
</details>

<details>
  <summary><h3>6. 컨텍스트 스위칭 시에는 어떤 일들이 일어나나요?</h3></summary>
<ul>
<li> 프로세스와 스레드는 컨텍스트 스위칭이 발생했을 때 어떤 차이가 있을까요?</li>
<li> 컨텍스트 스위칭이 발생할 때, 기존의 프로세스 정보는 커널스택에 어떠한 형식으로 저장되나요?</li>
<li> 컨텍스트 스위칭은 언제 일어날까요?</li>
</ul>
</details>

<details>
  <summary><h3>7. 프로세스 스케줄링 알고리즘에는 어떤 것들이 있나요?</h3></summary>
<ul>
<li> RR을 사용할 때, Time Slice에 따른 trade-off를 설명해 주세요.</li>
<li> 싱글 스레드 CPU 에서 상시로 돌아가야 하는 프로세스가 있다면, 어떤 스케쥴링 알고리즘을 사용하는 것이 좋을까요? 또 왜 그럴까요?</li>
<li> 동시성과 병렬성의 차이에 대해 설명해 주세요.</li>
<li> 타 스케쥴러와 비교하여, Multi-level Feedback Queue는 어떤 문제점들을 해결한다고 볼 수 있을까요?</li>
<li> FIFO 스케쥴러는 정말 쓸모가 없는 친구일까요? 어떤 시나리오에 사용하면 좋을까요? </li>
<li> 우리는 스케줄링 알고리즘을 "프로세스" 스케줄링 알고리즘이라고 부릅니다. 스레드는 다른 방식으로 스케줄링을 하나요?</li>
<li> 유저 스레드와 커널 스레드의 스케쥴링 알고리즘은 똑같을까요?</li>
</ul>
</details>

<details>
  <summary><h3>8. 뮤텍스와 세마포어의 차이점은 무엇인가요?</h3></summary>
<ul>
<li> 이진 세마포어와 뮤텍스의 차이에 대해 설명해 주세요.</li>
<li> Lock을 얻기 위해 대기하는 프로세스들은 Spin Lock 기법을 사용할 수 있습니다. 이 방법의 장단점은 무엇인가요? 단점을 해결할 방법은 없을까요?</li> 
<li> 뮤텍스와 세마포어 모두 커널이 관리하기 때문에, Lock을 얻고 방출하는 과정에서 시스템 콜을 호출해야 합니다. 이 방법의 장단점이 있을까요? 단점을 해결할 수 있는 방법은 없을까요?</li>
</ul>
</details>

<details>
  <summary><h3>9. Deadlock 에 대해 설명해 주세요.</h3></summary>
<ul>
<li> Deadlock 이 동작하기 위한 4가지 조건에 대해 설명해 주세요.</li>
<li> 그렇다면 3가지만 충족하면 왜 Deadlock 이 발생하지 않을까요?</li>
<li> 어떤 방식으로 예방할 수 있을까요?</li>
<li> 왜 현대 OS는 Deadlock을 처리하지 않을까요?</li>
<li> Wait Free와 Lock Free를 비교해 주세요.</li>
</ul>
</details>

<details>
  <summary><h3>10. 프로그램이 컴파일 되어, 실행되는 과정을 간략하게 설명해 주세요.</h3></summary>
<ul>
<li> 링커와, 로더의 차이에 대해 설명해 주세요.</li>
<li> 컴파일 언어와 인터프리터 언어의 차이에 대해 설명해 주세요.</li>
<li> JIT에 대해 설명해 주세요.</li>
<li> 본인이 사용하는 언어는, 어떤식으로 컴파일 및 실행되는지 설명해 주세요.</li>
<li> Python 같은 언어는 CPython, Jython, PyPy등의 다양한 구현체가 있습니다. 각각은 어떤 차이가 있을까요? 또한, 실행되는 과정 또한 다를까요?</li>
<li> 우리는 흔히 fork(), exec() 시스템 콜을 사용하여 프로세스를 적재할 수 있다고 배웠습니다. 로더의 역할은 이 시스템 콜과 상관있는 걸까요? 아니면 다른 방식으로 프로세스를 적재할 수 있는 건가요?</li>
</ul>
</details>

<details>
  <summary><h3>11. IPC가 무엇이고, 어떤 종류가 있는지 설명해 주세요.</h3></summary>
<ul>
<li> Shared Memory가 무엇이며, 사용할 때 유의해야 할 점에 대해 설명해 주세요.</li>
<li> 메시지 큐는 단방향이라고 할 수 있나요?</li>
</ul>
</details>

<details>
  <summary><h3>12. Thread Safe 하다는 것은 어떤 의미인가요?</h3></summary>
<ul>
<li> Thread Safe 를 보장하기 위해 어떤 방법을 사용할 수 있나요?</li>
<li> Peterson's Algorithm 이 무엇이며, 한계점에 대해 설명해 주세요.</li>
<li> Race Condition 이 무엇인가요?</li>
<li> Thread Safe를 구현하기 위해 반드시 락을 사용해야 할까요? 그렇지 않다면, 어떤 다른 방법이 있을까요?</li>
</ul>
</details>

<details>
  <summary><h3>13. Thread Pool, Monitor, Fork-Join에 대해 설명해 주세요.</h3></summary>
<ul>
<li> Thread Pool을 사용한다고 가정하면, 어떤 기준으로 스레드의 수를 결정할 것인가요? </li>
<li> 어떤 데이터를 정렬 하려고 합니다. 어떤 방식의 전략을 사용하는 것이 가장 안전하면서도 좋은 성능을 낼 수 있을까요?</li>
</ul>
</details>

<details>
  <summary><h3>14. 캐시 메모리 및 메모리 계층성에 대해 설명해 주세요.</h3></summary>
<ul>
<li> 캐시 메모리는 어디에 위치해 있나요?</li>
<li> L1, L2 캐시에 대해 설명해 주세요.</li>
<li> 캐시에 올라오는 데이터는 어떻게 관리되나요?</li>
<li> 캐시간의 동기화는 어떻게 이루어지나요?</li>
<li> 캐시 메모리의 Mapping 방식에 대해 설명해 주세요.</li>
<li> 캐시의 지역성에 대해 설명해 주세요.</li>
<li> 캐시의 지역성을 기반으로, 이차원 배열을 가로/세로로 탐색했을 때의 성능 차이에 대해 설명해 주세요.</li>
<li> 캐시의 공간 지역성은 어떻게 구현될 수 있을까요? (힌트: 캐시는 어떤 단위로 저장되고 관리될까요?) </li>
</ul>
</details>

<details>
  <summary><h3>15.메모리의 연속할당 방식 세 가지를 설명해주세요. (first-fit, best-fit, worst-fit)</h3></summary>
<ul>
<li> worst-fit 은 언제 사용할 수 있을까요?</li>
<li> 성능이 가장 좋은 알고리즘은 무엇일까요?</li>
</ul>
</details>

## 16. Thrashing 이란 무엇인가요?

<details>
<summary>Thrashing이란</summary>

- Thrashing은 프레임을 충분히 할당받지 못한 프로세스에서 과도한 페이징 작업이 일어나는 경우 발생함
- Thrashing으로 인해 Page Fault rate 증가 / CPU utilization 감소
- 다중 프로그래밍 정도가 높지 않을 때: CPU 수행을 하러 가던 프로세스가 I/O를 하러 가도, ready queue에 충분한 프로그램이 존재함
- 다중 프로그래밍 정도가 높을 때: 물리적 메모리에 너무 많은 프로그램이 존재하지만 필요한 만큼의 페이지가 존재하지 않아서 계속적인 Page Fault가 일어남
- 다중 프로그래밍(degree of multiprogramming): 메모리에 올라와 있는 프로그램 수를 의미함
<img width="292" height="173" alt="image" src="https://github.com/user-attachments/assets/540f5494-3ca6-4808-9133-f9c6b26a8e86" />

</details>

<details>
<summary>Thrashing 발생 시, 어떻게 완화할 수 있을까요?</summary>

1. **Working-Set Model**
   - 일정한 window size (Δ) 내에서 집중적으로 참조된 페이지 집합을 Locality 기반으로 정의하고, 이 집합 전체가 동시에 메모리에 적재돼야 프로세스가 원활히 실행됨
   - 만약 WS 전체를 수용할 프레임이 부족하면, 프로세스는 모든 프레임을 반납하고 swap-out 되며, 여유 프레임이 생기면 WS 단위로 다시 프레임을 할당받음
   - Window Size(Δ) 조정 필요: Δ가 너무 작으면 Locality 전체를 담지 못해 페이지 부재가 잦아지고, 너무 크면 잘 쓰이지 않는 페이지까지 포함돼 WS가 과대 추정되어 메모리를 낭비하고 다중 프로그래밍 정도가 불필요하게 낮아질 수 있음

2. **PFF(Page-Fault Frequency)**
   - Page Fault rate의 상한값(U)와 하한값(L)을 두고 프레임을 더 할당하거나 할당 프레임 수를 줄이는 방법
   - 빈 frame이 없으면 일부 프로세스를 swap out

</details>
  

## 17. 가상 메모리란 무엇인가요?

<details>
<summary>가상 메모리란</summary>

- 가상 메모리란 실제 물리 메모리(RAM)보다 큰 주소 공간을 프로세스에 제공하기 위해 사용되는 기술로, 실제로 접근되는 페이지만 물리 메모리에 동적으로 매핑·적재(Demand Paging)하여 프로세스가 사용할 수 있는 주소 범위를 물리 RAM 크기보다 크게 확장해 주는 메모리 관리 기법이다.
- 프로세스가 실제로는 물리 메모리의 제약을 받지 않고 마치 큰 메모리 공간을 가진 것처럼 동작할 수 있게 해준다.
- 여러 프로세스가 동시에 실행될 때 각각이 독립적인 가상 주소 공간을 가질 수 있어 메모리 격리와 보안을 제공한다.

</details>

<details>
<summary>가상 메모리가 가능한 이유가 무엇일까요?</summary>

- MMU(Memory-Management Unit)에 의해 logical address가 physical address로 매핑되기 때문임.
- 프로세스는 가상 주소만 알고 있고, 실제 물리 주소로의 변환은 하드웨어가 투명하게 처리한다.
- 페이지 테이블을 통해 가상 페이지와 물리 페이지 간의 매핑 정보를 관리하며, 필요에 따라 동적으로 매핑을 변경할 수 있다.

</details>

<details>
<summary>Page Fault가 발생했을 때, 어떻게 처리하는지 설명해 주세요.</summary>

1. **하드웨어 예외 발생**: MMU가 page fault trap을 발생시킴, CPU가 커널 모드로 전환
2. **커널 진입 & Page-Fault Handler 실행**: 인터럽트 벡터에 등록된 PF 핸들러로 점프
3. **Page Fault 처리**
   - 유효성 검사 진행(eg. bad address, protection violation)
   - 프레임 확보(없다면 페이지 교체 알고리즘으로 victim 선정)
   - 디스크 I/O, 해당 프로세스는 Block(Waiting) 상태로 전환, 다른 작업 실행
   - I/O 완료 인터럽트: 페이지-인 완료 처리 및 page tables entry 기록
   - 프로세스 재개 준비: Block → Ready 상태로 전환
4. **해당 프로세스 다시 running**
   - 동일한 명령어를 다시 실행하여 이번에는 메모리 접근이 성공하도록 한다.

</details>

<details>
<summary>페이지 크기에 대한 Trade-Off를 설명해 주세요.</summary>

**페이지 크기 감소**
- 페이지 수 증가, 페이지 테이블 크기 증가, 내부 단편(Internal fragmentation) 감소, 디스크 I/O 단위가 작고 필요한 정보만 메모리에 올라와 메모리 효율성 증가하지만 Locality 활용 측면에서는 좋지 않음
- TLB Miss가 더 자주 발생할 수 있으며, 페이지 테이블 관리 오버헤드가 증가한다.

**페이지 크기 증가** 
- 내부 단편 증가, 페이지 테이블 크기 감소, 연속 I/O 효율 증가, 메모리 낭비 가능성 커짐
- TLB Hit ratio가 향상되고 페이지 테이블 관리 오버헤드는 감소하지만, 프로세스가 실제로 사용하지 않는 데이터까지 메모리에 적재될 확률이 높아진다.

</details>

<details>
<summary>페이지 크기가 커지면, 페이지 폴트가 더 많이 발생한다고 할 수 있나요?</summary>

- 일반적으로 페이지 폴트 횟수는 감소하는 편(한 번의 I/O로 많은 데이터를 가져오기 때문)
- 다만 내부 단편이 증가해 Working Set을 수용하지 못해 Page Fault가 늘어나는 경우도 있음
- 공간 지역성(Spatial Locality)이 좋은 프로그램의 경우 페이지 크기가 클수록 Page Fault가 현저히 감소하지만, 랜덤 액세스 패턴을 가진 프로그램에서는 큰 효과를 보기 어렵다.

</details>

<details>
<summary>세그멘테이션 방식을 사용하고 있다면, 가상 메모리를 사용할 수 없을까요?</summary>

- 사용 가능, 세그멘테이션 방식 역시 가상 메모리 관리 기법 중 하나로, 가상 메모리를 논리적 단위인 세그먼트로 나누어 관리하는 방법임. (페이징: 동일한 크기 단위인 페이지로 분할)
- 페이징보다 세그멘테이션 방식이 메모리 효율적인 경우도 있음, Segmentation with Paging 방법도 있음
- 세그멘테이션은 프로그램의 논리적 구조(코드, 데이터, 스택 등)를 반영하여 메모리를 관리하므로 보호와 공유 측면에서 장점이 있다.
- 다만 가변 크기의 세그먼트로 인해 외부 단편화 문제가 발생할 수 있어, 이를 해결하기 위해 페이징과 결합한 방식이 많이 사용된다.

</details>

## 18. 세그멘테이션과 페이징의 차이점은 무엇인가요?
<details>
<summary>세그멘테이션과 페이징의 차이점은 무엇인가요?</summary>
  
**세그멘테이션**
- 프로그램을 의미 단위인 세그먼트로 나눔
- 세그먼트마다 다른 크기를 가짐
- 의미 단위로 관리하므로 코드 및 데이터보호가 쉬움 
- 외부 단편화 발생

**페이징**
- 메모리를 고정된 크기의 페이지로 나눈 것
- 모든 페이지 크기가 동일
- 메모리 관리가 단순해진다는 장점을 가짐
- 내부 단편화 발생 
</details>


<details>
<summary>페이지와 프레임의 차이에 대해 설명해 주세요.</summary>
  
**페이지**
- 프로세스의 가상 메모리를 고정된 크기로 나눈 단위
- 논리적 개념 

**프레임**
- 물리 메모리(RAM)을 동일한 크기 단위로나눈 블록
- 물리적 개념
- 페이지 테이블을 통해 페이지가 프레임에 매핑됨 

</details>

<details>
<summary>내부 단편화와, 외부 단편화에 대해 설명해 주세요.</summary>

**내부 단편화(Internal Fragmentation)**
- 할당된 메모리 블록 내부에서 사용되지 않는 공간이 발생
- 페이징에서 주로 발생: 프로세스가 필요한 메모리가 페이지 크기의 배수가 아닐 때, 마지막 페이지에서 사용되지 않는 공간이 생김

**외부 단편화(External Fragmentation)**
- 전체 메모리 공간은 충분하지만 연속된 공간이 부족해 할당할 수 없는 현상
- 세그멘테이션에서 주로 발생: 가변 크기의 세그먼트들이 할당/해제되면서 메모리 사이사이에 작은 빈 공간들이 생김
- 페이징에서는 고정 크기로 관리하므로 외부 단편화가 발생하지 않음

</details>

<details>
<summary>페이지에서 실제 주소를 어떻게 가져올 수 있는지 설명해 주세요.</summary>

- 가상 주소(VA): 페이지 번호(Page Number) + 페이지 오프셋(Page Offset)
- 페이지 번호를 인덱스로 사용해 페이지 테이블에서 해당하는 프레임 번호를 찾음
- 프레임 번호 + 페이지 오프셋 = 실제 물리 주소
- 보통 빠른 주소 변환을 위해 TLB에서 먼저 변환 정보를 찾고, 없으면 페이지 테이블을 조회함 

</details>

<details>
<summary>어떤 주소공간이 있을 때, 이 공간이 수정 가능한지 확인할 수 있는 방법이 있나요?</summary>

- 페이지 테이블 엔트리의 권한을 나타내는 Protection Bits를 확인
- R/W/X 권한을 나타내고 있음
- 하드웨어 레벨에서도 권한이 없는 접근 시도 시 Protection Fault 발생

</details>

<details>
<summary>32비트에서, 페이지의 크기가 1kb 이라면 페이지 테이블의 최대 크기는 몇 개일까요?</summary>


- 32비트 주소 공간 = 2^32 = 4GB
- 페이지 크기: 1KB = 2¹⁰ bytes
- 총 페이지 수 = 가상 주소 공간 / 페이지 크기 = 2^32 / 2^10 = 2^22 = 4,194,304 페이지
- 각 페이지 테이블 엔트리가 4바이트라고 가정하면
- 페이지 테이블 크기 = 4,194,304 × 4 bytes = 16MB

</details>

<details>
<summary>32비트 운영체제는 램을 최대 4G 까지 사용할 수 있습니다. 이 이유를 페이징과 연관 지어서 설명해 주세요.</summary>

- 32비트 주소는 2^32=4GB의 주소 공간만 표현 가능
- 가상 주소 -> 물리 주소 매핑 시, 최대 주소값이 
- 따라서 물리 메모리도 최대 4GB까지만 직접 주소 지정 가능

</details>

<details>
<summary>C/C++ 개발을 하게 되면 Segmentation Fault 라는 에러를 접할 수 있을텐데, 이 에러는 세그멘테이션/페이징과 어떤 관계가 있을까요?</summary>

- Segmentation Fault: 프로세스가 허용되지 않은 메모리 영역에 접근했을 때 발생
- 원인: 세그먼트 범위 밖 접근 (세그멘테이션), 페이지 권한 위반 (페이징)
- 페이징이 주류지만 전통적인 세그멘테이션 시스템에서는 세그먼트 경계를 벗어난 접근 시 발생
- C/C++에서 주로 발생하는 경우:NULL 포인터 접근, 해제된 메모리 접근, 배열 범위 초과
- 처리 과정:
  1. MMU가 보호 위반 감지: 페이지 테이블의 보호 비트를 확인함
  2. 하드웨어 예외 발생: Protection Fault trap 발생
  3. 커널의 시그널: 커널이 SIGSEGV 시그널을 프로세스에 전달
  4. 프로세스가 종료되며 segm
- [세그멘테이션 폴트](https://en.wikipedia.org/wiki/Segmentation_fault)
- [코어 덤프, 메모리 덤프](https://ko.wikipedia.org/wiki/%EC%BD%94%EC%96%B4_%EB%8D%A4%ED%94%84)
</details>


## 19. TLB는 무엇인가요?

<details>
<summary>TLB는 무엇인가요?</summary>
  
- Translation Look-aside Buffer
- 페이지 테이블의 최근 변환을 저장해 두는 하드웨어 캐시이며, page table 중 일부만 존재한다.
- TLB를 도입하게 되면 TLB를 우선적으로 살펴보고, 확률적으로 hit이 되기 때문에 miss일 때만 페이지 테이블을 살펴보면 된다.
- 전체 Translation Cost =  Cost of TLB Lookup + Prob(TLB miss) * Cost of PT Lookup

</details>

<details>
<summary>TLB를 쓰면 왜 빨라지나요?</summary>

- **메모리 접근 횟수**: 페이지 테이블 조회 < TLB 히트
- TLB는 모든 엔트리를 병렬로 비교해 일치하는 PTE가 있으면 즉시 히트 일반 SRAM이 아니라 CAM(Content-Addressable Memory) 구조를 사용 ⇒ Parallel Search, 속도 빠름
- 페이지 테이블이 메인 메모리에 저장되어 있어 접근 시 추가적인 메모리 참조가 필요하지만, TLB는 CPU 내부에 있어 매우 빠른 접근이 가능하다.

</details>

<details>
<summary>MMU가 무엇인가요?</summary>

- logical address를 physical address로 매핑해 주는 하드웨어 디바이스
- Page Fault 발생 시 trap을 걸어주는 것도 MMU
- 메모리 보호 기능도 담당하여 프로세스가 허용되지 않은 메모리 영역에 접근하려 할 때 예외를 발생시킨다.

</details>

<details>
<summary>TLB와 MMU는 어디에 위치해 있나요?</summary>

- TLB는 MMU 내부(CPU)에 위치
- MMU는 CPU와 메인 메모리 사이에서 주소 변환을 담당하며, 현대 CPU에서는 CPU 칩 내부에 통합되어 있다.

</details>

<details>
<summary>코어가 여러개라면, TLB는 어떻게 동기화 할 수 있을까요?</summary>

**멀티코어 환경에서의 TLB 동기화 문제점**
- 한 코어가 곧 작은 CPU, 동시에 독립 명령 흐름을 실행할 하드웨어 유닛이 여러 개
- 각 코어는 독립적인 TLB를 가지고 있어, 한 코어에서 페이지 테이블이 변경되면 다른 코어의 TLB에는 오래된 정보가 남아있을 수 있다.

**TLB Shootdown (IPI 기반)**
- 페이지 테이블을 수정하는 코어가 다른 모든 코어에게 IPI(Inter-Processor Interrupt)를 전송
- 받은 코어들은 해당 페이지에 대한 TLB 엔트리를 무효화(invalidate)
- 모든 코어가 응답할 때까지 기다린 후 페이지 테이블 수정을 완료
- 상당한 성능 오버헤드를 발생시킬 수 있음

</details>

<details>
<summary>TLB 관점에서, Context Switching 발생 시 어떤 변화가 발생하는지 설명해 주세요.</summary>

**고전적 방식**
- 프로세스 A → B 전환 → TLB 전체 flush (이전 프로세스 매핑 오염 방지).
- 단점: 첫 수 백 개 메모리 접근이 전부 TLB miss → 성능 손실.

**ASID / PCID 방식(현대 CPU)**
- TLB 엔트리에 Address-Space ID(ASID) 또는 Process-Context ID(PCID) 태그 부여.
- 컨텍스트 스위치 시 TLB lookup은 새로 시작되는 프로세스의 ASID와 TLB Cache에 남아있는 TLB entry의 ASID를 비교해서 매칭된 TLB enty만 사용한다.
- 컨텍스트 스위치 시 TLB flush를 생략하고 다른 프로세스 엔트리는 무시되므로 성능이 유지된다. 
- 커널이 *same-address-space* 재진입 시(예: Syscall) 재사용 가능.

</details>

<details>
<summary>참고 자료</summary>
  
- https://yohda.tistory.com/entry/%EB%A6%AC%EB%88%85%EC%8A%A4-%EC%BB%A4%EB%84%90-Scheduler-TLB
</details>

## 20. 동기화를 구현하기 위한 하드웨어적인 해결 방법에 대해 설명해 주세요.</h3></summary>

<details>
  <summary>volatile 키워드는 어떤 의미가 있나요?</summary>

  - volatile 키워드는 변수가 최적화에서 제외됨을 의미.
- CPU 캐시나 레지스터에 값이 상주하는 대신, 항상 메모리에서 직접 읽고 씀.
- 쓰레드 간 공유 변수나 메모리 맵 하드웨어 레지스터에서 사용함, 원자성이 필요한 경우에는 atomic 연산 또는 뮤텍스 필요
- 캐시 대신 항상 메모리를 통해 데이터 접근이 이루어짐으로 성능 저하 발생할 수 있음.
</details>

<details>
  <summary>v싱글코어가 아니라 멀티코어라면, 어떻게 동기화가 이뤄질까요?</summary>

- 뮤텍스, 세마포어: 운영체제나 프로그래밍 언어에서 제공하는 동기화 도구
- Diabling Interrupts: dlsxjfjqxmfmf RJtj lock을 구현 (문제 발생 가능, 멀티코어 시스템에서는 비효율적)
- Test-and-Set: 메모리 값 검사(test)와 설정(set)을 원자적으로 수행하는 CPU 명령어.(Atomic instruction)
- Compare-and-Swap (CAS): 메모리의 값이 기대한 값과 같으면 새 값으로 교체 (Atomic instruction)
- [캐시 일관성 프로토콜(MESI) ](https://velog.io/@prettylee620/%EB%A9%80%ED%8B%B0%EC%BD%94%EC%96%B4%EC%99%80-%EC%8A%A4%EB%A0%88%EB%93%9C%EC%99%80-%EC%9D%B8%ED%84%B0%EB%9F%BD%ED%8A%B8)

</details>



## 21. 페이지 교체 알고리즘에 대해 설명해 주세요.

<details>
  <summary>주요 페이지 교체 알고리즘</summary>

- **FIFO (First In First Out)**: 가장 먼저 들어온 페이지를 교체
- **LRU (Least Recently Used)**: 가장 오래 사용되지 않은 페이지를 교체
- **LFU (Least Frequently Used)**: 사용 빈도가 가장 낮은 페이지를 교체
- **Optimal**: 미래에 가장 늦게 참조될 페이지를 교체 (이론적 최적해)
- **Clock Algorithm**: LRU의 근사 알고리즘으로 참조 비트 사용
</details>

<details>
  <summary>LRU 알고리즘은 어떤 특성을 이용한 알고리즘이라고 할 수 있을까요?</summary>

- 최근에 참조된 페이지가 가까운 미래에 다시 참조될 가능성이 높다는 특성을 이용
- 단순한 참조 횟수보다는 언제 참조되었는지가 더 중요하다는 관점의 페이지 교체 알고리즘
</details>

<details>
  <summary>LRU 알고리즘을 구현한다면, 어떻게 구현할 수 있을까요?</summary>
- 연결 리스트+해시맵: 페이지를 이중 연결 리스트(Doubly Linked List)로 관리
- 스택(Linked List) 기반: 페이지를 참조할 때마다 스택의 최상단으로 이동 
</details>

<details>
  <summary>LRU 알고리즘의 단점을 설명해 주세요. 이를 해결할 수 있는 대안에 대해서도 설명해 주세요.</summary>  

- 정확한 LRU 구현을 위해선 하드웨어 지원이나 복잡한 자료구조 필요
- 소프트웨어로만 구현 시 참조 시마다 갱신 필요 -> 성능 저하 가능
- 최근 사용 패턴이 미래 사용과 항상 일치함을 보장할 수 없음
- LRU 근사 알고리즘: Clock 알고리즘, LRU-K(K번째 전 참조 시점), LFU (Least Frequently Used), Working Set Algorithm
</details>


## 22. File Descriptor와, File System에 에 대해 설명해 주세요.
<details>
  <summary>File Descriptor와, File System에 에 대해 설명해 주세요.</summary>
    
**File Descriptor**
- 운영체제가 프로세스와 열린 파일 사이를 구분하고 관리하기 위해 부여하는 정수 값
- 프로세스별로 관리되며, 0(stdin), 1(stdout), 2(stderr)은 기본적으로 할당됨
- 파일뿐만 아니라 소켓, 파이프 등 모든 I/O 리소스에 사용됨

**File System**
- 저장 장치(HDD, SSD 등)에 데이터를 구조적으로 조직하고 관리하는 방식
- 메타데이터 관리, 공간 할당, 접근 권한 제어, 파일 이름 관리 등의 기능

**File Descriptor와 File System의 관계**
- 프로세스 레벨에서 파일에 접근하기 위한 추상화된 인터페이스
- File System은 실제 저장 장치에서 파일 데이터와 메타데이터를 물리적으로 관리
- 시스템 콜을 통해 File Descriptor가 File System의 기능을 활용함

</details>

<details>
<summary> I-Node가 무엇인가요?</summary>
- Index Node의 줄임말로, 파일 시스템에서 파일이나 디렉토리의 메타데이터를 저장하는 자료구조
- 파일의 실제 내용이 아닌 파일에 대한 정보를 담고 있음
- I-Node 번호를 통해 해당 파일의 메타데이터와 실제 데이터 블록 위치를 찾음

<details>
<summary>프로그래밍 언어 상에서 제공하는 파일 관련 함수 (Python - open(), Java - BufferedReader/Writer 등)은, 파일을 어떤 방식으로 읽어들이나요?</summary>
</details>

## 23. 동기와 비동기, 블로킹과 논블로킹의 차이에 대해 설명해 주세요.</h3></summary>
<ul>
<li> 그렇다면, 동기이면서 논블로킹이고, 비동기이면서 블로킹인 경우는 의미가 있다고 할 수 있나요?</li>
<li> I/O 멀티플렉싱에 대해 설명해 주세요.</li>
<li> 논블로킹 I/O를 수행한다고 하면, 그 결과를 어떻게 수신할 수 있나요? </li>
</ul>
</details>
