\## 운영체제

### 1\. 시스템 콜이 무엇인지 설명해 주세요.

*   우리가 사용하는 시스템 콜의 예시를 들어주세요.
*   시스템 콜이, 운영체제에서 어떤 과정으로 실행되는지 설명해 주세요.
*   시스템 콜의 유형에 대해 설명해 주세요.
*   운영체제의 Dual Mode 에 대해 설명해 주세요.
*   왜 유저모드와 커널모드를 구분해야 하나요?
*   서로 다른 시스템 콜을 어떻게 구분할 수 있을까요?

### 2\. 인터럽트가 무엇인지 설명해 주세요.

*   인터럽트는 어떻게 처리하나요?
*   Polling 방식에 대해 설명해 주세요.
*   HW / SW 인터럽트에 대해 설명해 주세요.
*   동시에 두 개 이상의 인터럽트가 발생하면, 어떻게 처리해야 하나요?

### 3\. 프로세스가 무엇인가요?

*   프로그램과 프로세스, 스레드의 차이에 대해 설명해 주세요.
*   PCB가 무엇인가요?
*   그렇다면, 스레드는 PCB를 갖고 있을까요?
*   리눅스에서, 프로세스와 스레드는 각각 어떻게 생성될까요?
*   자식 프로세스가 상태를 알리지 않고 죽거나, 부모 프로세스가 먼저 죽게 되면 어떻게 처리하나요?
*   리눅스에서, 데몬프로세스에 대해 설명해 주세요.
*   리눅스는 프로세스가 일종의 트리를 형성하고 있습니다. 이 트리의 루트 노드에 위치하는 프로세스에 대해 설명해 주세요.

### 4\. 프로세스 주소공간에 대해 설명해 주세요.

*   초기화 하지 않은 변수들은 어디에 저장될까요?
*   일반적인 주소공간 그림처럼, Stack과 Heap의 크기는 매우 크다고 할 수 있을까요? 그렇지 않다면, 그 크기는 언제 결정될까요?
*   Stack과 Heap 공간에 대해, 접근 속도가 더 빠른 공간은 어디일까요?
*   다음과 같이 공간을 분할하는 이유가 있을까요?
*   스레드의 주소공간은 어떻게 구성되어 있을까요?
*   "스택"영역과 "힙"영역은 정말 자료구조의 스택/힙과 연관이 있는 걸까요? 만약 그렇다면, 각 주소공간의 동작과정과 연계해서 설명해 주세요.
*   IPC의 Shared Memory 기법은 프로세스 주소공간의 어디에 들어가나요? 그런 이유가 있을까요?
*   스택과 힙영역의 크기는 언제 결정되나요? 프로그램 개발자가 아닌, 사용자가 이 공간의 크기를 수정할 수 있나요?

### 5\. 단기, 중기, 장기 스케쥴러에 대해 설명해 주세요.

*   현대 OS에는 단기, 중기, 장기 스케쥴러를 모두 사용하고 있나요?
*   프로세스의 스케쥴링 상태에 대해 설명해 주세요.
*   preemptive/non-preemptive 에서 존재할 수 없는 상태가 있을까요?
*   Memory가 부족할 경우, Process는 어떠한 상태로 변화할까요?

### 6\. 컨텍스트 스위칭 시에는 어떤 일들이 일어나나요?

*   프로세스와 스레드는 컨텍스트 스위칭이 발생했을 때 어떤 차이가 있을까요?
*   컨텍스트 스위칭이 발생할 때, 기존의 프로세스 정보는 커널스택에 어떠한 형식으로 저장되나요?
*   컨텍스트 스위칭은 언제 일어날까요?

### 7\. 프로세스 스케줄링 알고리즘에는 어떤 것들이 있나요?

*   RR을 사용할 때, Time Slice에 따른 trade-off를 설명해 주세요.
*   싱글 스레드 CPU 에서 상시로 돌아가야 하는 프로세스가 있다면, 어떤 스케쥴링 알고리즘을 사용하는 것이 좋을까요? 또 왜 그럴까요?
*   동시성과 병렬성의 차이에 대해 설명해 주세요.
*   타 스케쥴러와 비교하여, Multi-level Feedback Queue는 어떤 문제점들을 해결한다고 볼 수 있을까요?
*   FIFO 스케쥴러는 정말 쓸모가 없는 친구일까요? 어떤 시나리오에 사용하면 좋을까요?
*   우리는 스케줄링 알고리즘을 "프로세스" 스케줄링 알고리즘이라고 부릅니다. 스레드는 다른 방식으로 스케줄링을 하나요?
*   유저 스레드와 커널 스레드의 스케쥴링 알고리즘은 똑같을까요?

### 8\. 뮤텍스와 세마포어의 차이점은 무엇인가요?

*   이진 세마포어와 뮤텍스의 차이에 대해 설명해 주세요.
*   Lock을 얻기 위해 대기하는 프로세스들은 Spin Lock 기법을 사용할 수 있습니다. 이 방법의 장단점은 무엇인가요? 단점을 해결할 방법은 없을까요?
*   뮤텍스와 세마포어 모두 커널이 관리하기 때문에, Lock을 얻고 방출하는 과정에서 시스템 콜을 호출해야 합니다. 이 방법의 장단점이 있을까요? 단점을 해결할 수 있는 방법은 없을까요?

### 9\. Deadlock 에 대해 설명해 주세요.

*   Deadlock 이 동작하기 위한 4가지 조건에 대해 설명해 주세요.
*   그렇다면 3가지만 충족하면 왜 Deadlock 이 발생하지 않을까요?
*   어떤 방식으로 예방할 수 있을까요?
*   왜 현대 OS는 Deadlock을 처리하지 않을까요?
*   Wait Free와 Lock Free를 비교해 주세요.

### 10\. 프로그램이 컴파일 되어, 실행되는 과정을 간략하게 설명해 주세요.

*   링커와, 로더의 차이에 대해 설명해 주세요.
*   컴파일 언어와 인터프리터 언어의 차이에 대해 설명해 주세요.
*   JIT에 대해 설명해 주세요.
*   본인이 사용하는 언어는, 어떤식으로 컴파일 및 실행되는지 설명해 주세요.
*   Python 같은 언어는 CPython, Jython, PyPy등의 다양한 구현체가 있습니다. 각각은 어떤 차이가 있을까요? 또한, 실행되는 과정 또한 다를까요?
*   우리는 흔히 fork(), exec() 시스템 콜을 사용하여 프로세스를 적재할 수 있다고 배웠습니다. 로더의 역할은 이 시스템 콜과 상관있는 걸까요? 아니면 다른 방식으로 프로세스를 적재할 수 있는 건가요?

### 11\. IPC가 무엇이고, 어떤 종류가 있는지 설명해 주세요.

*   Shared Memory가 무엇이며, 사용할 때 유의해야 할 점에 대해 설명해 주세요.
*   메시지 큐는 단방향이라고 할 수 있나요?

### 12\. Thread Safe 하다는 것은 어떤 의미인가요?

*   Thread Safe 를 보장하기 위해 어떤 방법을 사용할 수 있나요?
*   Peterson's Algorithm 이 무엇이며, 한계점에 대해 설명해 주세요.
*   Race Condition 이 무엇인가요?
*   Thread Safe를 구현하기 위해 반드시 락을 사용해야 할까요? 그렇지 않다면, 어떤 다른 방법이 있을까요?

### 13\. Thread Pool, Monitor, Fork-Join에 대해 설명해 주세요.

*   Thread Pool을 사용한다고 가정하면, 어떤 기준으로 스레드의 수를 결정할 것인가요?
*   어떤 데이터를 정렬 하려고 합니다. 어떤 방식의 전략을 사용하는 것이 가장 안전하면서도 좋은 성능을 낼 수 있을까요?

### 14\. 캐시 메모리 및 메모리 계층성에 대해 설명해 주세요.

*   캐시 메모리는 어디에 위치해 있나요?
*   L1, L2 캐시에 대해 설명해 주세요.
*   캐시에 올라오는 데이터는 어떻게 관리되나요?
*   캐시간의 동기화는 어떻게 이루어지나요?
*   캐시 메모리의 Mapping 방식에 대해 설명해 주세요.
*   캐시의 지역성에 대해 설명해 주세요.
*   캐시의 지역성을 기반으로, 이차원 배열을 가로/세로로 탐색했을 때의 성능 차이에 대해 설명해 주세요.
*   캐시의 공간 지역성은 어떻게 구현될 수 있을까요? (힌트: 캐시는 어떤 단위로 저장되고 관리될까요?)

### 15.메모리의 연속할당 방식 세 가지를 설명해주세요. (first-fit, best-fit, worst-fit)

*   worst-fit 은 언제 사용할 수 있을까요?
*   성능이 가장 좋은 알고리즘은 무엇일까요?

### 16\. Thrashing 이란 무엇인가요?

운영체제에서 Page fault가 과도하게 발생해 CPU가 실제 연산보다 페이지 교체 작업에 대부분의 시간을 소모하는 상태를 말합니다. 주로 멀티프로그래밍 환경에서 각 프로세스에 할당된 물리 메모리(frame)가 해당 프로세스의 지역성(locality)을 유지하기 부족할 때 발생합니다.
> - MPD 증가에 따라 CPU 사용률도 증가되다가 어느 순간 급격하게 감소하는 그래프
> - 프로세스가 원활한 수행에 필요한 최소한의 page frame 수를 할당받지 못함 **→** Page fault rate이 매우 높아짐 → CPU 사용률 감소 → OS는 MPD(Multiprogramming degree)를 높여야 한다고 판단 → 또 다른 프로세스가 시스템에 추가됨 → 프로세스당 할당된 frame의 수가 더욱 감소→ 프로세스는 page의 swap in/swap out(디스크 I/O)으로 매우 바쁨 → 대부분의 시간에 CPU는 한가함 → 전체 처리량 감소

*   Thrashing 발생 시, 어떻게 완화할 수 있을까요?

> 1. **Working Set Model**
    - 프로세스가 일정 시간 동안 원활하게 수행되기 위해 한꺼번에 메모리에 올라와 있어야 하는 페이지 집합, 즉 Working Set의 최소 프레임 수를 보장해 지역성을 유지합니다.(Working Set Window)
        - 참조의 지역성: 프로세스는 특정 시간 동안 일정 장소만을 집중적으로 참조
> 2. **Page Fault Frequency(PFF) Scheme**
    - 페이지 부재율이 상한을 초과하면 프레임을 추가하고, 하한 미만이면 프레임을 회수해 메모리 사용을 균형화하는 방식입니다.
> 3. **멀티프로그래밍 수준(MPL) 조절**
    - 동시에 실행하는 프로세스 수를 줄여 프로세스당 할당 프레임 수를 확보합니다.

### 17\. 가상 메모리란 무엇인가요?

> - 운영체제가 프로세스 주소 공간을 페이지 단위로 나누어 필요할 때만 물리 메모리에 적재하고 나머지는 보조기억장치(디스크)에 저장하는 메모리 관리 기법입니다. 프로세스에 실제 물리 메모리보다 큰 주소 공간을 제공하기 위함입니다. 이를 통해 프로그램은 연속된 큰 메모리 공간을 사용하는 것처럼 동작하고, 메모 보호와 프로세스 간 격리가 가능합니다.

*   가상 메모리가 가능한 이유가 무엇일까요?
>  - 지역성 때문입니다. (시간 지역성-최근에 참조한 메모리 영역을 가까운 미래에도 참조할 가능성이 높고, 공간 지역성-현재 참조한 메모리 주소와 인접한 주소를 곧 참조할 가능성이 높습니다.) 이 때문에 전체 페이지 중 일부만 메모리에 있어도 실행이 가능하며, 나머지는 필요할 때 보조기억장치에서 로드할 수 있습니다.

*   Page Fault가 발생했을 때, 어떻게 처리하는지 설명해 주세요.
>  1. **CPU**: 해당 페이지가 메모리에 없음을 감지, Page Fault trap 발생 → OS에 제어권 이양
>  2. **OS Page Fault Handler**:
    - 참조한 페이지의 유효성 검사 (잘못된 접근 시 프로세스 종료)
    - 물리 메모리에 빈 프레임이 있는지 확인
        - 있으면 디스크에서 해당 페이지 로드
        - 없으면 페이지 교체 알고리즘(FIFO, LRU 등)으로 교체 대상 페이지 선택 후 스왑 아웃
>  3. **페이지 테이블 갱신:**  새 프레임 번호, 유효 비트 설정.
>  4. **프로세스 재시작** : 중단된 명령어부터 재실행.

*   페이지 크기에 대한 Trade-Off를 설명해 주세요.
>  - 페이지 테이블 크기: "페이지 수 × 엔트리 크기"
>  - 페이지 크기가 커지면 → 페이지 테이블 엔트리 수가 감소해 페이지 테이블 크기가 감소하고, 메모리 오버헤드가 감소하고, 디스크 I/O 효율이 증가합니다. / 그러나 내부 단편화가 증가하고 지역성이 저하될 수 있습니다.
>  - 페이지 크기가 작아지면  → 내부 단편화가 줄어들고 지역성 유지에 유리합니다. / 그러나 필요한 페이지 수가 증가해, 페이지 테이블 크기가 커지고 페이지 폴트 처리 횟수가 증가할 수 있습니다.

*   페이지 크기가 커지면, 페이지 폴트가 더 많이 발생한다고 할 수 있나요?
>  - 페이지 크기가 커질수록 페이지 폴트 비율은 특정 구간까지는 증가하지만, 그 이후로는 감소하는 양상을 보입니다. 페이지 크기가 커질수록 물리 메모리에 들어갈 수 있는 페이지의 개수가 작아져서 페이지 폴트가 증가하게 되는 것인데, 페이지가 매우 커지면 대부분의 경우에 하나의 페이지 안에 필요한 다음 페이지가 들어있기 때문에 페이지 폴트가 감소합니다. (공간 지역성)
>  - 프로그램이 데이터 순차 접근 많은지 / 다양한 위치 불규칙 접근 많은지에 따라서도 차이

*   세그멘테이션 방식을 사용하고 있다면, 가상 메모리를 사용할 수 없을까요?
>  - 사용할 수 있습니다. 필요한 세그먼트만 메모리에 적재하는 방식으로 구현할 수 있습니다. 그리고 페이징과 세그멘테이션을 혼합해 세그먼트를 페이지 단위로 나누어 관리할 수 있습니다.

### 18\. 세그멘테이션과 페이징의 차이점은 무엇인가요?

> **세그멘테이션(Segmentation)**
> - **단위**: 논리적 단위(코드, 데이터, 스택 등)로 나눈 세그먼트.
>  - **크기**: 세그먼트마다 가변 길이.
>  - **주소 구조**: `(세그먼트 번호, 오프셋)`
>  - **장점**: 코드·데이터 등 논리적 단위별 보호와 공유가 용이, 내부 단편화 없음.
>  - **단점**: 가변 길이로 인해 외부 단편화 발생, 세그먼트 테이블 필요.
> 
>  **페이징(Paging)**
>  - **단위**: 고정 크기의 페이지.
>  - **크기**: 모든 페이지 동일.
>  - **주소 구조**: `(페이지 번호, 오프셋)`
>  - **장점**: 외부 단편화 없음, 메모리 관리 단순.
>  - **단점**: 페이지 크기에 따라 내부 단편화 발생, 페이지 테이블로 인한 메모리 오버헤드.
> 
>  **차이점 정리**
>  - **단위**: 세그멘테이션은 논리적 단위, 페이징은 물리적 관리 단위.
> - **크기**: 세그먼트는 가변, 페이지는 고정.
> - **단편화**: 세그멘테이션은 외부 단편화, 페이징은 내부 단편화 발생.
> - **보호·공유**: 세그멘테이션이 논리적 단위별로 더 유연.
> 

*   페이지와 프레임의 차이에 대해 설명해 주세요.

> 페이지(Page)는 **가상 메모리**를 일정한 크기의 블록으로 나눈 단위이고, 프레임(Frame)은 **물리 메모리**를 동일한 크기의 블록으로 나눈 단위입니다.
> 
> 프로세스 실행 시, 페이지 단위로 잘린 가상 주소 공간의 각 페이지가 물리 메모리의 빈 프레임에 매핑되어 적재됩니다. 두 단위의 크기는 동일하게 설정되어 주소 변환과 메모리 관리가 단순해집니다.
> 
> 프레임 할당은 OS 커널이 물리 메모리에서, 페이지 할당은 OS가 페이지 테이블을 통해 가상→물리 매핑을 구성하는 과정에서 수행됩니다.
> 
*   내부 단편화와, 외부 단편화에 대해 설명해 주세요.
> **내부 단편화(Internal Fragmentation)**
> - 고정 크기 블록(예: 페이지)을 할당했을 때, **할당된 블록 내부에서 사용되지 않고 남는 공간**이 발생하는 현상입니다.
> - 원인: 프로세스나 데이터 크기가 블록 크기보다 작을 때 발생.
> - 예시: 4KB 페이지에 3.7KB 데이터만 저장하면 0.3KB가 내부 단편화.
> 
> **외부 단편화(External Fragmentation)**
> - 가변 크기 메모리 할당에서, **총 여유 공간은 충분하지만 연속된 큰 공간이 없어 할당이 불가능한 현상**입니다.
> - 원인: 할당과 해제가 반복되면서 여유 공간이 잘게 쪼개질 때 발생.
> - 예시: 10KB 여유 공간이 3KB, 4KB, 3KB로 흩어져 있어 5KB 요청을 처리하지 못하는 경우.

*   페이지에서 실제 주소를 어떻게 가져올 수 있는지 설명해 주세요.

> 페이지 기반 가상 메모리에서 실제 주소(물리 주소)를 가져오는 과정은 **주소 변환(Address Translation)**이며, **MMU(Memory Management Unit)**가 수행합니다.
> 
> 1. **가상 주소 분리**
>     - 가상 주소(Virtual Address)는 **페이지 번호(Page Number)**와 **페이지 오프셋(Page Offset)**으로 나뉩니다.
>     - 상위 비트 → 페이지 번호, 하위 비트 → 페이지 내 오프셋.
> 2. **페이지 테이블 조회**
>     - 페이지 번호를 인덱스로 하여 **페이지 테이블(Page Table)**에서 해당 페이지의 물리 프레임 번호(Frame Number)를 찾습니다.
>     - 페이지 테이블 엔트리에는 유효 비트, 프레임 번호, 접근 권한 등이 저장됩니다.
> 3. **물리 주소 생성**
>     - 페이지 테이블에서 얻은 프레임 번호를 물리 주소의 상위 비트로 사용하고,
>     - 가상 주소의 페이지 오프셋을 그대로 하위 비트로 붙여 물리 주소를 완성합니다.
> 4. **TLB 최적화**
>     - 매번 페이지 테이블을 조회하는 오버헤드를 줄이기 위해, 최근 변환 결과를 **TLB(Translation Lookaside Buffer)**에 캐싱하여 빠르게 물리 주소를 얻습니다.

*   어떤 주소공간이 있을 때, 이 공간이 수정 가능한지 확인할 수 있는 방법이 있나요?
> 페이지 테이블 엔트리(Page Table Entry, PTE)의 Protection Bit와 OS가 제공하는 메모리 정보 API를 통해 확인합니다. 운영체제는 각 주소 공간에 대해 PTE에 Protection Bit를 설정해 두며, 이 비트를 통해 해당 메모리 영역이 수정 가능한지 여부를 판단합니다.

*   32비트에서, 페이지의 크기가 1kb 이라면 페이지 테이블의 최대 크기는 몇 개일까요?
> 32비트 OS의 주소 범위는 2³²B = 4GB입니다. 단일 레벨 페이지 테이블에서 페이지 크기가 1KB = 2¹⁰B이면, 페이지 수 = 전체 주소 공간 ÷ 페이지 크기 = 2³² ÷ 2¹⁰ = 2²² 페이지, 따라서 2²²개입니다.

*   32비트 운영체제는 램을 최대 4G 까지 사용할 수 있습니다. 이 이유를 페이징과 연관 지어서 설명해 주세요.
> 32비트 주소 공간은 주소를 나타내는 데 32비트를 사용하므로, 표현 가능한 주소 수는 2³²개입니다.
> 각 주소는 1바이트를 가리키므로 최대 주소 가능 용량은 2³²B = 4GB* 입니다.
> 페이징을 사용하면 이 32비트 가상 주소가 (페이지 번호 + 페이지 오프셋) 으로 나뉘어 물리 주소로 변환되지만, 가상 주소 자체의 비트 수가 한계이므로 매핑 가능한 물리 메모리 크기도 4GB를 넘을 수 없습니다.

*   C/C++ 개발을 하게 되면 Segmentation Fault 라는 에러를 접할 수 있을텐데, 이 에러는 세그멘테이션/페이징과 어떤 관계가 있을까요?
> Segmentation Fault는 프로세스가 접근 권한이 없는 메모리 영역에 접근했을 때 발생하는 하드웨어 예외입니다. 과거 세그멘테이션 메모리 관리 시절에서 유래했지만, 현대 시스템에서는 페이지 테이블 접근 권한 위반이나 잘못된 가상 주소 접근에서도 동일한 에러 명칭을 사용합니다.

### 19\. TLB는 무엇인가요?
> TLB(Translation Lookaside Buffer)는 페이지 테이블 조회 속도를 높이기 위해 최근의 가상→물리 주소 변환 결과를 캐싱하는 고속 메모리입니다. 주소 변환의 캐시 역할을 하여 페이징 시스템에서 발생하는 주소 변환 오버헤드를 최소화합니다.

*   TLB를 쓰면 왜 빨라지나요?
> 페이지 테이블 조회를 캐시로 대체해 주소 변환의 병목을 줄이는 역할을 합니다.

*   MMU가 무엇인가요?
> CPU와 메모리 사이에서 프로그램이 사용하는 가상 주소를 물리 주소로 변환하고, 접근 권한을 검사하는 하드웨어 장치입니다.(CPU 안에 포함) 주소 변환 시 페이지 테이블과 TLB를 활용하며, 불법 접근 시 페이지 폴트를 발생시켜 OS가 처리하도록 합니다.

*   TLB와 MMU는 어디에 위치해 있나요?
> CPU 내부에 MMU, MMU 내부에 TLB가 있습니다.

*   코어가 여러개라면, TLB는 어떻게 동기화 할 수 있을까요?
> 멀티코어 환경에서 TLB는 코어별로 독립되어 있어(로컬 캐시) 페이지 테이블 변경 시 **TLB Shootdown** 이 필요합니다. OS는 변경을 감지한 코어에서 다른 코어로 **IPI(Inter-Processor Interrupt)**를 보내 TLB 무효화를 요청합니다. 각 코어는 해당 엔트리(또는 전체)를 flush하고, 이후 접근 시 새 변환 정보를 페이지 테이블에서 다시 로드합니다.

*   TLB 관점에서, Context Switching 발생 시 어떤 변화가 발생하는지 설명해 주세요.
> Context Switching 시에는 실행 중인 프로세스가 바뀌므로, 기존 프로세스의 가상 주소 공간과 관련된 **TLB 엔트리들이 더 이상 유효하지 않게 됩니다**. <br> 이 때문에 대부분의 OS는 Context Switch 직후 **TLB를 flush(무효화)**하여 새로운 프로세스의 페이지 테이블에 맞는 주소 변환이 이루어지도록 합니다. <br> 다만, CPU가 **Address Space Identifier(ASID)**를 지원하면, 서로 다른 프로세스의 TLB 엔트리를 구분해 저장할 수 있어 전체 flush 없이도 Context Switch 오버헤드를 줄일 수 있습니다.

### 20\. 동기화를 구현하기 위한 하드웨어적인 해결 방법에 대해 설명해 주세요.

> 하드웨어 차원에서 동기화를 구현하는 대표적인 방법은 **원자적 연산(Atomic Operation)을 보장하는 명령어**를 제공하는 것입니다.
> 
> - **Test-and-Set**: 메모리 위치 값을 읽고, 1로 설정하는 작업을 원자적으로 수행해 임계구역 진입 여부를 판단합니다.
> - **Compare-and-Swap(CAS)**: 지정한 메모리 값이 기대값과 같으면 새 값으로 교체하는 연산을 원자적으로 수행합니다.
> - **Exchange(Swap)**: 두 변수의 값을 원자적으로 교환합니다.
> - **Load-Linked / Store-Conditional (LL/SC)**: 읽은 값이 변경되지 않았을 때만 쓰기를 허용하여 경쟁 상태를 방지합니다.

*   volatile 키워드는 어떤 의미가 있나요?
> volatile 키워드는 변수가 컴파일러 최적화에서 제외되어, 항상 메모리에서 직접 읽고 쓰도록 보장하는 의미를 가집니다.

*   싱글코어가 아니라 멀티코어라면, 어떻게 동기화가 이뤄질까요?
> 멀티코어 환경에서는 각 코어가 **별도의 캐시와 레지스터**를 가지므로, 단순한 명령어 실행만으로는 메모리 값이 일관되지 않을 수 있습니다. 이 때문에 하드웨어와 OS는 다음 메커니즘을 통해 동기화를 보장합니다.
> 
> 1. **캐시 일관성 프로토콜(Cache Coherence Protocol)**
>     - MESI, MOESI 같은 프로토콜로 코어 간 캐시 상태를 동기화하여, 한 코어의 쓰기 변경이 다른 코어에서도 즉시 반영되도록 합니다.
> 2. **메모리 배리어(Memory Barrier)**
>     - CPU가 명령어 실행 순서를 재배치하지 않도록 막아, 쓰기/읽기 순서를 보장합니다.
> 3. **원자적 연산 명령어 + 버스 락**
>     - `Test-and-Set`, `Compare-and-Swap(CAS)` 등의 원자적 명령어를 실행할 때, CPU는 버스 락 또는 캐시 락을 걸어 다중 코어 접근 충돌을 방지합니다.

### 21\. 페이지 교체 알고리즘에 대해 설명해 주세요.
> 페이지 교체 알고리즘은 가상 메모리에서 새로운 페이지를 적재하려 하지만 남은 메모리가 없을 때, 기존의 어떤 페이지를 교체할지 결정하는 알고리즘입니다.

*   LRU 알고리즘은 어떤 특성을 이용한 알고리즘이라고 할 수 있을까요?

> LRU(Least Recently Used) 알고리즘은 시간 지역성(Temporal Locality) 특성을 이용합니다. 즉, 최근에 참조된 페이지는 가까운 미래에도 다시 참조될 가능성이 높고, 반대로 오래 참조되지 않은 페이지는 앞으로도 참조될 가능성이 낮다는 점을 활용합니다.

*   LRU 알고리즘을 구현한다면, 어떻게 구현할 수 있을까요?

> 자바로 해시 맵과 이중 연결 리스트를 결합해 구현할 수 있습니다. (참고: [LRU 캐시 알고리즘의 이해와 구현 | F-lab](https://f-lab.kr/insight/understanding-lru-cache-20240605?gad_source=2))
> ```java
> import java.util.HashMap;
> import java.util.Map;
> import java.util.LinkedList;
> 
> public class LRUCache {
>     private final int capacity;
>     private final Map cache;
>     private final LinkedList order;
> 
>     public LRUCache(int capacity) {
>         this.capacity = capacity;
>         this.cache = new HashMap<>();
>         this.order = new LinkedList<>();
>     }
> 
>     public int get(int key) {
>         if (!cache.containsKey(key)) {
>             return -1;
>         }
>         order.remove((Integer) key);
>         order.addFirst(key);
>         return cache.get(key);
>     }
> 
>     public void put(int key, int value) {
>         if (cache.containsKey(key)) {
>             order.remove((Integer) key);
>         } else if (cache.size() == capacity) {
>             int oldest = order.removeLast();
>             cache.remove(oldest);
>         }
>         cache.put(key, value);
>         order.addFirst(key);
>     }
> }
> ```

*   LRU 알고리즘의 단점을 설명해 주세요. 이를 해결할 수 있는 대안에 대해서도 설명해 주세요.
>   1. **메모리, 시간(연산) 오버헤드가 있습니다.** 데이터의 접근 시간을 기록하고 갱신하기 위해 추가적인 메모리와 연산이 필요합니다. (캐시의 크기와 데이터 접근 빈도에 따라 성능에 영향; But, LRU 알고리즘의 장점인 빠른 데이터 접근 속도는 오버헤드를 상쇄하는 이점)
>   2. **주기적으로 사용되는 데이터가 캐시에서 불필요하게 제거될 수 있습니다.** 특정 데이터가 주기적으로 사용되지만, 그 주기가 길다면 해당 데이터가 캐시에서 제거되는 것과 같은 특정 상황에서 성능이 저하될 수 있습니다.
> - 대안:
> 1. LFU (Least Frequently Used)
> - 가장 적게 참조된 데이터를 제거하는 방법입니다.
> - 접근 빈도 기반으로 캐시 유지
> - 장점: 주기적 사용 데이터 유지에 유리
> - 단점: 구현 복잡도 증가, 오래 전에 자주 사용된 데이터가 남을 수 있음
> 2. ARC (Adaptive Replacement Cache)
> - LRU와 LFU를 동적으로 조합
> - 실제 워크로드에 따라 자동으로 비율 조정
> - 장점: 일관된 성능, 적응력 높음
> - 단점: 구현 난이도 높음
> 3. LRU-K
> - 최근 K번째 접근 시간을 기준으로 판단
> - 짧은 주기의 잡음성 접근을 제거하고, 장기적으로 주기적인 데이터 유지에 강함
> - 단점: 접근 기록을 더 많이 저장해야 하므로 메모리 사용량 증가

### 22\. File Descriptor와, File System에 대해 설명해 주세요.
>   **File Descriptor(FD)**: 리눅스/유닉스 계열의 시스템에서 프로세스가 특정 파일에 접근할 때 사용하는 추상적인 값입니다. 일반적으로 0이 아닌 정수값을 가집니다. File Descriptor 0, 1, 2는 프로세스가 시작될 때 커널에 의해 미리 선점되어 있습니다. 이후 파일을 열면 FD 3번부터 순차적으로 할당됩니다.
> - FD 번호 0: stdin
> - FD 번호 1: stdout
> - FD 번호 2: stderr
> 
> 파일을 열었을 때의 동작:
>   1. 프로세스가 파일을 열면, 커널은 해당 프로세스의 'File Descriptor Table'에서 아직 사용되지 않은 가장 작은 정수 인덱스 번호를 찾아 새로 연 파일에 할당한다. 이 번호가 FD이다.
>   2. 이후 프로세스는 read(fd, ...), write(fd, ...), close(fd) 같은 **시스템 콜을 호출할 때, 해당 FD 값을 인자로 전달함으로써 해당 파일을 참조**하게 된다. 즉, FD는 열려 있는 파일에 대한 핸들처럼 동작한다.
>   예시:
>   ```c
>   int fd = open("data.txt", O_RDONLY);
>   read(fd, buffer, size);  // 'data.txt'에서 데이터를 읽기
>   close(fd);               // 파일 닫기
>   ```

*   I-Node가 무엇인가요?
> - 유닉스/리눅스 파일 시스템에서 파일이나 디렉토리에 대한 **메타데이터와 데이터 위치 정보**를 저장하는 데이터 구조.
>   - 담고 있는 정보: (파일 크기, 소유자, 그룹, 권한, 생성 시간, 수정 시간, 마지막 접근 시간 등)
>   - inode는 파일명과는 별도로 관리된다. 파일명은 '디렉토리 엔트리'에서 inode 번호와 매핑되어 관리되어 관리된다.
>   - 파일 이름은 inode를 가리키는 포인터인 것. inode에 메타데이터를 집중시켜 관리하므로, 파일명을 바꿔도 inode는 그대로 유지되고 효율적인 관리가 가능하다. <br>
>   - inode는 파일 내용을 담은 하나 이상의 데이터 블록 주소를 직접/간접 포인터로 저장한다.
>   - 하드 링크: 하나의 inode를 여러 디렉토리 엔트리가 참조할 수 있게 함

*   프로그래밍 언어 상에서 제공하는 파일 관련 함수 (Python - open(), Java - BufferedReader/Writer 등)은, 파일을 어떤 방식으로 읽어들이나요?
>   - 프로그래밍 언어에서 제공하는 파일 입출력 함수들은 **시스템 콜**을 추상화한 고수준 API이다.
>   - 파일을 읽는 방식 흐름
>   1. **파일 열기 (open() 등)**: 내부적으로 open() 시스템 콜을 호출해 FD 획득, 커널은 이 FD를 통해 파일 inode 및 데이터 블록에 접근
>   2. **버퍼링 (Buffering)**: 언어나 API에 따라 내부 버퍼를 사용하여 **한 번에 여러 바이트를 읽고 메모리에 보관.** 성능 향상을 위해 **OS I/O 호출을 줄이는 목적**.
>   3. **파일 읽기 (read(), readLine() 등)**: 사용자 요청에 따라 버퍼에서 필요한 양만큼 반환. 버퍼가 비면, 다시 OS에 시스템 콜을 호출해 디스크에서 데이터 블록을 읽어옴
>   4. **파일 닫기 (close())**: 내부 버퍼 비우기(flush) 및 FD 반납 (close() 시스템 콜)
>   ```c
>   f = open("data.txt", "r")  # 내부 버퍼 크기 예: 8192 bytes
>   chunk1 = f.read(100)       # 버퍼에 8192바이트 로드 후 그중 100바이트 반환
>   chunk2 = f.read(100)       # 버퍼에 남은 데이터에서 100바이트 반환
>   ...
>   ```

### 23\. 동기와 비동기, 블로킹과 논블로킹의 차이에 대해 설명해 주세요.
> - **동기와 비동기 - 제어 흐름 관점에서 구분**
>   - 동기: 작업이 끝날 때까지 호출자가 기다린 후 다음 작업을 수행합니다.
>   - 비동기: 작업을 요청한 뒤 즉시 반환하며, 완료 여부는 콜백, 이벤트, Future 등의 방식으로 나중에 통지받습니다.
>       - 콜백(Callback): 함수 실행이 끝나면 **지정한 함수(콜백)**를 호출해 결과를 전달
>       - 이벤트(Event): 작업 완료 시 이벤트를 발생시켜 핸들러가 응답
>       - Future(또는 Promise): 비동기 작업 결과를 나중에 조회할 수 있는 객체
> - **블로킹과 논블로킹 - 스레드 상태 관점에서 구분**
>   - 블로킹: 작업이 완료될 때까지 스레드가 중단됩니다.
>   - 논블로킹: 바로 반환되어 스레드가 다른 작업을 계속할 수 있습니다.

*   그렇다면, 동기이면서 논블로킹이고, 비동기이면서 블로킹인 경우는 의미가 있다고 할 수 있나요?
>   - 대체로 각각의 강점을 살릴 수 없어 실용성이 낮습니다. 주로 특수 목적이나 과도기적 상황에서만 사용됩니다. 일반적으로는 동기+블로킹 또는 비동기+논블로킹 조합이 권장됩니다.
>   - 동기 + 논블로킹: 호출자는 결과를 즉시 받지만, 직접 반복 확인(polling)해야 합니다. 의미는 있지만 비효율적이라 실제로는 잘 쓰이지 않습니다.
>   - 비동기 + 블로킹: 호출은 비동기지만, 결과를 받을 때 get()처럼 블로킹 호출을 하면 결국 동기처럼 동작합니다.(예: Java Future.get(); / Future: 비동기 작업의 결과를 나중에 참조할 수 있는 객체)

*   I/O 멀티플렉싱에 대해 설명해 주세요.
>   하나의 스레드 또는 프로세스가 동시에 여러 I/O 채널(예: 소켓, 파일 등)을 비동기적으로 감시하고, 준비된 채널에 대해서만 처리하는 방식입니다.
> 이 방식은 불필요한 블로킹을 피하고, 적은 수의 스레드로 많은 연결을 효율적으로 처리할 수 있어, 고성능 네트워크 서버나 이벤트 기반 시스템에서 주로 사용됩니다. <br>
> 리눅스에서는 select(), poll(), epoll() 등이 대표적인 시스템 콜이고, 그 중 epoll()은 이벤트 기반으로 동작하며, 수천 개의 연결에서도 성능이 유지되어 널리 사용됩니다. <br>
> 실제로 Nginx, Node.js, Java NIO, Netty 같은 프레임워크들도 I/O 멀티플렉싱 기반으로 동작합니다. <br>
> 즉, 멀티스레드 대신 이벤트 루프 기반 구조로 많은 I/O를 효율적으로 처리하는 핵심 기술입니다.

*   논블로킹 I/O를 수행한다고 하면, 그 결과를 어떻게 수신할 수 있나요?
>   폴링 (Polling):  반복적으로 상태를 확인하여 결과가 준비됐는지 검사. 비효율적이지만 단순한 방식 콜백 (Callback) 결과가 준비되면 등록된 콜백 함수가 자동 실행됩니다. 이벤트 기반 프레임워크나 라이브러리에서 사용됩니다. (ex. Node.js, Java NIO 비동기 채널)
